{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition video stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the shape_predictor_68_face_landmarks.dat file from:\n",
    " [shape_predictor_68_face_landmarks.dat.bz2](http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "/home/ubuntu/opencv/opencv/modules/imgproc/src/color.cpp:9748: error: (-215) scn == 3 || scn == 4 in function cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-da637ab6a1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_capture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mclahe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateCLAHE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipLimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtileGridSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mclahe_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclahe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /home/ubuntu/opencv/opencv/modules/imgproc/src/color.cpp:9748: error: (-215) scn == 3 || scn == 4 in function cvtColor\n"
     ]
    }
   ],
   "source": [
    "#Import required modules\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "#Set up some required objects\n",
    "#Webcam object\n",
    "video_capture = cv2.VideoCapture(0) \n",
    "#Face detector\n",
    "detector = dlib.get_frontal_face_detector() \n",
    "#Landmark identifier. Set the filename to whatever you named the downloaded file\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    clahe_image = clahe.apply(gray)\n",
    "\n",
    "    #Detect the faces in the image\n",
    "    detections = detector(clahe_image, 1)\n",
    "    \n",
    "    #For each detected face\n",
    "    for k,d in enumerate(detections): \n",
    "        #Get coordinates\n",
    "        shape = predictor(clahe_image, d) \n",
    "        #There are 68 landmark points on each face\n",
    "        for i in range(1,68): \n",
    "            #For each point, draw a red circle with thickness2 on the original frame\n",
    "            cv2.circle(frame, (shape.part(i).x, shape.part(i).y), 1, (0,0,255), thickness=2) \n",
    "    cv2.imshow(\"image\", frame) #Display the frame\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): #Exit program when the user presses 'q'\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extracting features from the faces\n",
    "to transform these nice dots overlaid on your face into features to feed the classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2, glob, random, math, numpy as np, dlib, itertools\n",
    "\n",
    "#Emotion list\n",
    "emotions = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "# emotions = [\"neutral\", \"anger\", \"disgust\", \"happy\", \"surprise\"]\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "#Define function to get file list, randomly shuffle it and split 80/20\n",
    "def get_files(emotion):\n",
    "    files = glob.glob(\"dataset/%s/*\" %emotion)\n",
    "    random.shuffle(files)\n",
    "    training = files[:int(len(files)*0.8)] #get first 80% of file list\n",
    "    prediction = files[-int(len(files)*0.2):] #get last 20% of file list\n",
    "    return training, prediction\n",
    "\n",
    "def get_landmarks(image):\n",
    "    detections = detector(image, 1)\n",
    "    #For all detected face instances individually\n",
    "    for k,d in enumerate(detections):\n",
    "        #Draw Facial Landmarks with the predictor class\n",
    "        shape = predictor(image, d)\n",
    "        xlist = []\n",
    "        ylist = []\n",
    "        #Store X and Y coordinates in two lists\n",
    "        for i in range(1,68):\n",
    "            xlist.append(float(shape.part(i).x))\n",
    "            ylist.append(float(shape.part(i).y))\n",
    "        \n",
    "        #Get the mean of both axes to determine centre of gravity\n",
    "        xmean = np.mean(xlist)\n",
    "        ymean = np.mean(ylist)\n",
    "        #get distance between each point and the central point in both axes\n",
    "        xcentral = [(x-xmean) for x in xlist]\n",
    "        ycentral = [(y-ymean) for y in ylist]\n",
    "\n",
    "        #If x-coordinates of the set are the same, the angle is 0, \n",
    "        #catch to prevent 'divide by 0' error in function\n",
    "        if xlist[26] == xlist[29]:\n",
    "            anglenose = 0\n",
    "        else:\n",
    "            anglenose = int(math.atan((ylist[26]-ylist[29])/(xlist[26]-xlist[29]))*180/math.pi)\n",
    "\n",
    "        if anglenose < 0:\n",
    "            anglenose += 90\n",
    "        else:\n",
    "            anglenose -= 90\n",
    "\n",
    "        landmarks_vectorised = []\n",
    "        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\n",
    "            landmarks_vectorised.append(x)\n",
    "            landmarks_vectorised.append(y)\n",
    "            meannp = np.asarray((ymean,xmean))\n",
    "            coornp = np.asarray((z,w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            anglerelative = (math.atan((z-ymean)/(w-xmean))*180/math.pi) - anglenose\n",
    "            landmarks_vectorised.append(dist)\n",
    "            landmarks_vectorised.append(anglerelative)\n",
    "\n",
    "    if len(detections) < 1: \n",
    "        landmarks_vectorised = \"error\"\n",
    "    return landmarks_vectorised\n",
    "\n",
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "    for emotion in emotions:\n",
    "        training, prediction = get_files(emotion)\n",
    "        #Append data to training and prediction list, and generate labels 0-7\n",
    "        for item in training:\n",
    "            #open image\n",
    "            image = cv2.imread(item)\n",
    "            #convert to grayscale\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            clahe_image = clahe.apply(gray)\n",
    "            landmarks_vectorised = get_landmarks(clahe_image)\n",
    "            if landmarks_vectorised == \"error\":\n",
    "                pass\n",
    "            else:\n",
    "                #append image array to training data list\n",
    "                training_data.append(landmarks_vectorised)\n",
    "                training_labels.append(emotions.index(emotion))\n",
    "    \n",
    "        for item in prediction:\n",
    "            image = cv2.imread(item)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            clahe_image = clahe.apply(gray)\n",
    "            landmarks_vectorised = get_landmarks(clahe_image)\n",
    "            if landmarks_vectorised == \"error\":\n",
    "                pass\n",
    "            else:\n",
    "                prediction_data.append(landmarks_vectorised)\n",
    "                prediction_labels.append(emotions.index(emotion))\n",
    "\n",
    "    return training_data, training_labels, prediction_data, prediction_labels   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sets 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:61: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training SVM linear 0\n",
      "getting accuracies 0\n",
      "linear:  0.90350877193\n",
      "Making sets 1\n",
      "training SVM linear 1\n",
      "getting accuracies 1\n",
      "linear:  0.868421052632\n",
      "Making sets 2\n",
      "training SVM linear 2\n",
      "getting accuracies 2\n",
      "linear:  0.90350877193\n",
      "Making sets 3\n",
      "training SVM linear 3\n",
      "getting accuracies 3\n",
      "linear:  0.885964912281\n",
      "Making sets 4\n",
      "training SVM linear 4\n",
      "getting accuracies 4\n",
      "linear:  0.912280701754\n",
      "Making sets 5\n",
      "training SVM linear 5\n",
      "getting accuracies 5\n",
      "linear:  0.885964912281\n",
      "Making sets 6\n",
      "training SVM linear 6\n",
      "getting accuracies 6\n",
      "linear:  0.912280701754\n",
      "Making sets 7\n",
      "training SVM linear 7\n",
      "getting accuracies 7\n",
      "linear:  0.885964912281\n",
      "Making sets 8\n",
      "training SVM linear 8\n",
      "getting accuracies 8\n",
      "linear:  0.90350877193\n",
      "Making sets 9\n",
      "training SVM linear 9\n",
      "getting accuracies 9\n",
      "linear:  0.877192982456\n",
      "Mean value lin svm: 0.894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#Set the classifier as a support vector machines with polynomial kernel\n",
    "clf = SVC(kernel='linear', probability=True, tol=1e-3)\n",
    "\n",
    "accur_lin = []\n",
    "for i in range(0,10):\n",
    "    #Make sets by random sampling 80/20%\n",
    "    print(\"Making sets %s\" %i)\n",
    "    training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "\n",
    "    npar_train = np.array(training_data) #Turn the training set into a numpy array for the classifier\n",
    "    npar_trainlabs = np.array(training_labels)\n",
    "    print(\"training SVM linear %s\" %i) #train SVM\n",
    "    clf.fit(npar_train, training_labels)\n",
    "\n",
    "    print(\"getting accuracies %s\" %i) #Use score() function to get accuracy\n",
    "    npar_pred = np.array(prediction_data)\n",
    "    pred_lin = clf.score(npar_pred, prediction_labels)\n",
    "    print \"linear: \", pred_lin\n",
    "    accur_lin.append(pred_lin) #Store accuracy in a list\n",
    "\n",
    "#Get mean accuracy of the 10 runs\n",
    "print(\"Mean value lin svm: %.3f\" %np.mean(accur_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sets 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training SVM polynomial 0\n",
      "getting accuracies 0\n",
      "linear:  0.795275590551\n",
      "Making sets 1\n",
      "training SVM polynomial 1\n",
      "getting accuracies 1\n",
      "linear:  0.874015748031\n",
      "Making sets 2\n",
      "training SVM polynomial 2\n",
      "getting accuracies 2\n",
      "linear:  0.795275590551\n",
      "Making sets 3\n",
      "training SVM polynomial 3\n",
      "getting accuracies 3\n",
      "linear:  0.803149606299\n",
      "Making sets 4\n",
      "training SVM polynomial 4\n",
      "getting accuracies 4\n",
      "linear:  0.771653543307\n",
      "Making sets 5\n",
      "training SVM polynomial 5\n",
      "getting accuracies 5\n",
      "linear:  0.826771653543\n",
      "Making sets 6\n",
      "training SVM polynomial 6\n",
      "getting accuracies 6\n",
      "linear:  0.763779527559\n",
      "Making sets 7\n",
      "training SVM polynomial 7\n",
      "getting accuracies 7\n",
      "linear:  0.763779527559\n",
      "Making sets 8\n",
      "training SVM polynomial 8\n",
      "getting accuracies 8\n",
      "linear:  0.834645669291\n",
      "Making sets 9\n",
      "training SVM polynomial 9\n",
      "getting accuracies 9\n",
      "linear:  0.858267716535\n",
      "Mean value poly svm: 0.809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#Set the classifier as a support vector machines with polynomial kernel\n",
    "clf = SVC(kernel='poly', probability=True, tol=1e-3)\n",
    "\n",
    "accur_lin = []\n",
    "for i in range(0,10):\n",
    "    #Make sets by random sampling 80/20%\n",
    "    print(\"Making sets %s\" %i)\n",
    "    training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "\n",
    "    npar_train = np.array(training_data) #Turn the training set into a numpy array for the classifier\n",
    "    npar_trainlabs = np.array(training_labels)\n",
    "    print(\"training SVM polynomial %s\" %i) #train SVM\n",
    "    clf.fit(npar_train, training_labels)\n",
    "\n",
    "    print(\"getting accuracies %s\" %i) #Use score() function to get accuracy\n",
    "    npar_pred = np.array(prediction_data)\n",
    "    pred_lin = clf.score(npar_pred, prediction_labels)\n",
    "    print \"linear: \", pred_lin\n",
    "    accur_lin.append(pred_lin) #Store accuracy in a list\n",
    "\n",
    "#Get mean accuracy of the 10 runs\n",
    "print(\"Mean value poly svm: %.3f\" %np.mean(accur_lin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sets 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:58: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training  0\n",
      "getting accuracies 0\n",
      "gaussianNB:  0.377952755906\n",
      "Making sets 1\n",
      "training  1\n",
      "getting accuracies 1\n",
      "gaussianNB:  0.551181102362\n",
      "Making sets 2\n",
      "training  2\n",
      "getting accuracies 2\n",
      "gaussianNB:  0.606299212598\n",
      "Making sets 3\n",
      "training  3\n",
      "getting accuracies 3\n",
      "gaussianNB:  0.393700787402\n",
      "Making sets 4\n",
      "training  4\n",
      "getting accuracies 4\n",
      "gaussianNB:  0.283464566929\n",
      "Making sets 5\n",
      "training  5\n",
      "getting accuracies 5\n",
      "gaussianNB:  0.511811023622\n",
      "Making sets 6\n",
      "training  6\n",
      "getting accuracies 6\n",
      "gaussianNB:  0.51968503937\n",
      "Making sets 7\n",
      "training  7\n",
      "getting accuracies 7\n",
      "gaussianNB:  0.377952755906\n",
      "Making sets 8\n",
      "training  8\n",
      "getting accuracies 8\n",
      "gaussianNB:  0.40157480315\n",
      "Making sets 9\n",
      "training  9\n",
      "getting accuracies 9\n",
      "gaussianNB:  0.51968503937\n",
      "Mean value gaussinNB: 0.454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Set the classifier as a gaussian navie bayes\n",
    "clf = GaussianNB()\n",
    "\n",
    "accur_lin = []\n",
    "for i in range(0,10):\n",
    "    #Make sets by random sampling 80/20%\n",
    "    print(\"Making sets %s\" %i)\n",
    "    training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "\n",
    "    npar_train = np.array(training_data) #Turn the training set into a numpy array for the classifier\n",
    "    npar_trainlabs = np.array(training_labels)\n",
    "    print(\"training  %s\" %i) #train NB\n",
    "    clf.fit(npar_train, training_labels)\n",
    "\n",
    "    print(\"getting accuracies %s\" %i) #Use score() function to get accuracy\n",
    "    npar_pred = np.array(prediction_data)\n",
    "    pred_lin = clf.score(npar_pred, prediction_labels)\n",
    "    print \"gaussianNB: \", pred_lin\n",
    "    accur_lin.append(pred_lin) #Store accuracy in a list\n",
    "\n",
    "#Get mean accuracy of the 10 runs\n",
    "print(\"Mean value gaussinNB: %.3f\" %np.mean(accur_lin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sets 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:58: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training  0\n",
      "getting accuracies 0\n",
      "Random forest:  0.732283464567\n",
      "Making sets 1\n",
      "training  1\n",
      "getting accuracies 1\n",
      "Random forest:  0.763779527559\n",
      "Making sets 2\n",
      "training  2\n",
      "getting accuracies 2\n",
      "Random forest:  0.740157480315\n",
      "Making sets 3\n",
      "training  3\n",
      "getting accuracies 3\n",
      "Random forest:  0.779527559055\n",
      "Making sets 4\n",
      "training  4\n",
      "getting accuracies 4\n",
      "Random forest:  0.732283464567\n",
      "Making sets 5\n",
      "training  5\n",
      "getting accuracies 5\n",
      "Random forest:  0.732283464567\n",
      "Making sets 6\n",
      "training  6\n",
      "getting accuracies 6\n",
      "Random forest:  0.748031496063\n",
      "Making sets 7\n",
      "training  7\n",
      "getting accuracies 7\n",
      "Random forest:  0.811023622047\n",
      "Making sets 8\n",
      "training  8\n",
      "getting accuracies 8\n",
      "Random forest:  0.748031496063\n",
      "Making sets 9\n",
      "training  9\n",
      "getting accuracies 9\n",
      "Random forest:  0.755905511811\n",
      "Mean value Random forest: 0.754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Set the classifier as a random forest \n",
    "clf = RandomForestClassifier(n_estimators=7)\n",
    "\n",
    "accur_lin = []\n",
    "for i in range(0,10):\n",
    "    #Make sets by random sampling 80/20%\n",
    "    print(\"Making sets %s\" %i)\n",
    "    training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "\n",
    "    npar_train = np.array(training_data) #Turn the training set into a numpy array for the classifier\n",
    "    npar_trainlabs = np.array(training_labels)\n",
    "    print(\"training  %s\" %i) #train Random forest\n",
    "    clf.fit(npar_train, training_labels)\n",
    "\n",
    "    print(\"getting accuracies %s\" %i) #Use score() function to get accuracy\n",
    "    npar_pred = np.array(prediction_data)\n",
    "    pred_lin = clf.score(npar_pred, prediction_labels)\n",
    "    print \"Random forest: \", pred_lin\n",
    "    accur_lin.append(pred_lin) #Store accuracy in a list\n",
    "\n",
    "#Get mean accuracy of the 10 runs\n",
    "print(\"Mean value Random forest: %.3f\" %np.mean(accur_lin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sets 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:58: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training  0\n",
      "getting accuracies 0\n",
      "Random forest:  0.51968503937\n",
      "Making sets 1\n",
      "training  1\n",
      "getting accuracies 1\n",
      "Random forest:  0.574803149606\n",
      "Making sets 2\n",
      "training  2\n",
      "getting accuracies 2\n",
      "Random forest:  0.582677165354\n",
      "Making sets 3\n",
      "training  3\n",
      "getting accuracies 3\n",
      "Random forest:  0.582677165354\n",
      "Making sets 4\n",
      "training  4\n",
      "getting accuracies 4\n",
      "Random forest:  0.614173228346\n",
      "Making sets 5\n",
      "training  5\n",
      "getting accuracies 5\n",
      "Random forest:  0.622047244094\n",
      "Making sets 6\n",
      "training  6\n",
      "getting accuracies 6\n",
      "Random forest:  0.606299212598\n",
      "Making sets 7\n",
      "training  7\n",
      "getting accuracies 7\n",
      "Random forest:  0.59842519685\n",
      "Making sets 8\n",
      "training  8\n",
      "getting accuracies 8\n",
      "Random forest:  0.59842519685\n",
      "Making sets 9\n",
      "training  9\n",
      "getting accuracies 9\n",
      "Random forest:  0.551181102362\n",
      "Mean value Random forest: 0.585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Set the classifier as a random forest \n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "accur_lin = []\n",
    "for i in range(0,10):\n",
    "    #Make sets by random sampling 80/20%\n",
    "    print(\"Making sets %s\" %i)\n",
    "    training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "\n",
    "    npar_train = np.array(training_data) #Turn the training set into a numpy array for the classifier\n",
    "    npar_trainlabs = np.array(training_labels)\n",
    "    print(\"training  %s\" %i) #train Random forest\n",
    "    clf.fit(npar_train, training_labels)\n",
    "\n",
    "    print(\"getting accuracies %s\" %i) #Use score() function to get accuracy\n",
    "    npar_pred = np.array(prediction_data)\n",
    "    pred_lin = clf.score(npar_pred, prediction_labels)\n",
    "    print \"Random forest: \", pred_lin\n",
    "    accur_lin.append(pred_lin) #Store accuracy in a list\n",
    "\n",
    "#Get mean accuracy of the 10 runs\n",
    "print(\"Mean value Random forest: %.3f\" %np.mean(accur_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:61: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SVC_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create svm model\n",
    "# emotion lists\n",
    "emotions = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "\n",
    "# create a clahe from cv2\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "# using delib detector and predictor to get face landmarks\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "#Set the classifier as a support vector machines with polynomial kernel\n",
    "clf = SVC(kernel='linear', probability=True, tol=1e-3)\n",
    "\n",
    "data, data_labels = [], []\n",
    "\n",
    "for emotion in emotions:\n",
    "    files = glob.glob(\"dataset/%s/*\" % emotion)\n",
    "    #Append data to training and prediction list, and generate labels 0-7\n",
    "    for item in files:\n",
    "        #open image\n",
    "        image = cv2.imread(item)\n",
    "        #convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        clahe_image = clahe.apply(gray)\n",
    "        landmarks_vectorised = get_landmarks(clahe_image)\n",
    "        if landmarks_vectorised == \"error\":\n",
    "            pass\n",
    "        else:\n",
    "            #append image array to training data list\n",
    "            data.append(landmarks_vectorised)\n",
    "            data_labels.append(emotions.index(emotion))\n",
    "\n",
    "npar_train = np.array(data) #Turn the training set into a numpy array for the classifier\n",
    "npar_trainlabs = np.array(data_labels)\n",
    "clf.fit(npar_train, npar_trainlabs)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'SVC_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf2 = joblib.load('SVC_model.pkl')\n",
    "clf2.p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
