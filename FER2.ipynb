{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition video stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the shape_predictor_68_face_landmarks.dat file from:\n",
    " [shape_predictor_68_face_landmarks.dat.bz2](http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0db8b08077e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclahe_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#Draw Facial Landmarks with the predictor class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mxlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#Store X and Y coordinates in two lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "#Import required modules\n",
    "import cv2, glob, random, math, numpy as np, dlib, itertools\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "emotions = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "\n",
    "#Set up some required objects\n",
    "#Webcam object\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "ret, frame = video_capture.read()\n",
    "# width = np.size(frame, 1)\n",
    "# height = np.size(frame, 0)\n",
    "#Face detector\n",
    "detector = dlib.get_frontal_face_detector() \n",
    "#Landmark identifier. Set the filename to whatever you named the downloaded file\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "# load facial expression model SVM\n",
    "clf2 = joblib.load('SVC_model.pkl')\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    clahe_image = clahe.apply(gray)\n",
    "\n",
    "    #Detect the faces in the image\n",
    "    detections = detector(clahe_image, 1)\n",
    "    \n",
    "    #For each detected face\n",
    "    for k,d in enumerate(detections): \n",
    "        #Get coordinates\n",
    "        shape = predictor(clahe_image, d) \n",
    "        #Draw Facial Landmarks with the predictor class\n",
    "        xlist, ylist = [], []\n",
    "        #Store X and Y coordinates in two lists\n",
    "        for i in range(1,68):\n",
    "            xlist.append(float(shape.part(i).x))\n",
    "            ylist.append(float(shape.part(i).y))\n",
    "            #For each point, draw a red circle with thickness2 on the original frame\n",
    "            cv2.circle(frame, (shape.part(i).x, shape.part(i).y), 1, (0,0,255), thickness=2)\n",
    "        \n",
    "        #Get the mean of both axes to determine centre of gravity\n",
    "        xmean, ymean = np.mean(xlist), np.mean(ylist)\n",
    "        #get distance between each point and the central point in both axes\n",
    "        xcentral = [(x-xmean) for x in xlist]\n",
    "        ycentral = [(y-ymean) for y in ylist]\n",
    "\n",
    "        #If x-coordinates of the set are the same, the angle is 0, \n",
    "        #catch to prevent 'divide by 0' error in function\n",
    "        if xlist[26] == xlist[29]:\n",
    "            anglenose = 0\n",
    "        else:\n",
    "            anglenose = int(math.atan((ylist[26]-ylist[29])/(xlist[26]-xlist[29]))*180/math.pi)\n",
    "\n",
    "        if anglenose < 0:\n",
    "            anglenose += 90\n",
    "        else:\n",
    "            anglenose -= 90\n",
    "\n",
    "        landmarks_vectorised = []\n",
    "        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\n",
    "            landmarks_vectorised.append(x)\n",
    "            landmarks_vectorised.append(y)\n",
    "            meannp = np.asarray((ymean,xmean))\n",
    "            coornp = np.asarray((z,w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            anglerelative = (math.atan((z-ymean)/(w-xmean))*180/math.pi) - anglenose\n",
    "            landmarks_vectorised.append(dist)\n",
    "            landmarks_vectorised.append(anglerelative)\n",
    "\n",
    "        ind = clf2.predictor(landmarks_vectorised)  \n",
    "        #show facial expression on the screen\n",
    "        cv2.putText(frame, emotions[ind], (10, 10), cv2.FONT_HERSHEY_PLAIN, 1.0, (0,255,0), thickness=1)\n",
    "\n",
    "    cv2.imshow(\"image\", frame) #Display the frame\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): #Exit program when the user presses 'q'\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extracting features from the faces\n",
    "to transform these nice dots overlaid on your face into features to feed the classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2, glob, random, math, numpy as np, dlib, itertools\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Emotion list\n",
    "emotions = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "# emotions = [\"neutral\", \"anger\", \"disgust\", \"happy\", \"surprise\"]\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "#Set the classifier as a support vector machines with polynomial kernel\n",
    "clf = SVC(kernel='linear', probability=True, tol=1e-3)\n",
    "\n",
    "#Define function to get file list, randomly shuffle it and split 80/20\n",
    "def get_files(emotion):\n",
    "    files = glob.glob(\"dataset/%s/*\" %emotion)\n",
    "    random.shuffle(files)\n",
    "    training = files[:int(len(files)*0.8)] #get first 80% of file list\n",
    "    prediction = files[-int(len(files)*0.2):] #get last 20% of file list\n",
    "    return training, prediction\n",
    "\n",
    "def get_landmarks(image):\n",
    "    detections = detector(image, 1)\n",
    "    #For all detected face instances individually\n",
    "    for k,d in enumerate(detections):\n",
    "        #Draw Facial Landmarks with the predictor class\n",
    "        shape = predictor(image, d)\n",
    "        xlist = []\n",
    "        ylist = []\n",
    "        #Store X and Y coordinates in two lists\n",
    "        for i in range(1,68):\n",
    "            xlist.append(float(shape.part(i).x))\n",
    "            ylist.append(float(shape.part(i).y))\n",
    "        \n",
    "        #Get the mean of both axes to determine centre of gravity\n",
    "        xmean = np.mean(xlist)\n",
    "        ymean = np.mean(ylist)\n",
    "        #get distance between each point and the central point in both axes\n",
    "        xcentral = [(x-xmean) for x in xlist]\n",
    "        ycentral = [(y-ymean) for y in ylist]\n",
    "\n",
    "        #If x-coordinates of the set are the same, the angle is 0, \n",
    "        #catch to prevent 'divide by 0' error in function\n",
    "        if xlist[26] == xlist[29]:\n",
    "            anglenose = 0\n",
    "        else:\n",
    "            anglenose = int(math.atan((ylist[26]-ylist[29])/(xlist[26]-xlist[29]))*180/math.pi)\n",
    "\n",
    "        if anglenose < 0:\n",
    "            anglenose += 90\n",
    "        else:\n",
    "            anglenose -= 90\n",
    "\n",
    "        landmarks_vectorised = []\n",
    "        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\n",
    "            landmarks_vectorised.append(x)\n",
    "            landmarks_vectorised.append(y)\n",
    "            meannp = np.asarray((ymean,xmean))\n",
    "            coornp = np.asarray((z,w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            anglerelative = (math.atan((z-ymean)/(w-xmean))*180/math.pi) - anglenose\n",
    "            landmarks_vectorised.append(dist)\n",
    "            landmarks_vectorised.append(anglerelative)\n",
    "\n",
    "    if len(detections) < 1: \n",
    "        landmarks_vectorised = \"error\"\n",
    "    return landmarks_vectorised\n",
    "\n",
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "    for emotion in emotions:\n",
    "        training, prediction = get_files(emotion)\n",
    "        #Append data to training and prediction list, and generate labels 0-7\n",
    "        for item in training:\n",
    "            #open image\n",
    "            image = cv2.imread(item)\n",
    "            #convert to grayscale\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            clahe_image = clahe.apply(gray)\n",
    "            landmarks_vectorised = get_landmarks(clahe_image)\n",
    "            if landmarks_vectorised == \"error\":\n",
    "                pass\n",
    "            else:\n",
    "                #append image array to training data list\n",
    "                training_data.append(landmarks_vectorised)\n",
    "                training_labels.append(emotions.index(emotion))\n",
    "    \n",
    "        for item in prediction:\n",
    "            image = cv2.imread(item)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            clahe_image = clahe.apply(gray)\n",
    "            landmarks_vectorised = get_landmarks(clahe_image)\n",
    "            if landmarks_vectorised == \"error\":\n",
    "                pass\n",
    "            else:\n",
    "                prediction_data.append(landmarks_vectorised)\n",
    "                prediction_labels.append(emotions.index(emotion))\n",
    "\n",
    "    return training_data, training_labels, prediction_data, prediction_labels   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sets 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:61: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training SVM linear 0\n",
      "getting accuracies 0\n",
      "linear:  0.964912280702\n",
      "Making sets 1\n",
      "training SVM linear 1\n",
      "getting accuracies 1\n",
      "linear:  0.877192982456\n",
      "Making sets 2\n",
      "training SVM linear 2\n",
      "getting accuracies 2\n",
      "linear:  0.877192982456\n",
      "Making sets 3\n",
      "training SVM linear 3\n",
      "getting accuracies 3\n",
      "linear:  0.921052631579\n",
      "Making sets 4\n",
      "training SVM linear 4\n",
      "getting accuracies 4\n",
      "linear:  0.90350877193\n",
      "Making sets 5\n",
      "training SVM linear 5\n",
      "getting accuracies 5\n",
      "linear:  0.90350877193\n",
      "Making sets 6\n",
      "training SVM linear 6\n",
      "getting accuracies 6\n",
      "linear:  0.885964912281\n",
      "Making sets 7\n",
      "training SVM linear 7\n",
      "getting accuracies 7\n",
      "linear:  0.894736842105\n",
      "Making sets 8\n",
      "training SVM linear 8\n",
      "getting accuracies 8\n",
      "linear:  0.938596491228\n",
      "Making sets 9\n",
      "training SVM linear 9\n",
      "getting accuracies 9\n",
      "linear:  0.929824561404\n",
      "Mean value lin svm: 0.910\n"
     ]
    }
   ],
   "source": [
    "accur_lin = []\n",
    "for i in range(0,10):\n",
    "    #Make sets by random sampling 80/20%\n",
    "    print(\"Making sets %s\" %i)\n",
    "    training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "\n",
    "    npar_train = np.array(training_data) #Turn the training set into a numpy array for the classifier\n",
    "    npar_trainlabs = np.array(training_labels)\n",
    "    print(\"training SVM linear %s\" %i) #train SVM\n",
    "    clf.fit(npar_train, training_labels)\n",
    "\n",
    "    print(\"getting accuracies %s\" %i) #Use score() function to get accuracy\n",
    "    npar_pred = np.array(prediction_data)\n",
    "    pred_lin = clf.score(npar_pred, prediction_labels)\n",
    "    print \"linear: \", pred_lin\n",
    "    accur_lin.append(pred_lin) #Store accuracy in a list\n",
    "\n",
    "#Get mean accuracy of the 10 runs\n",
    "print(\"Mean value lin svm: %.3f\" %np.mean(accur_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:61: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SVC_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create svm model\n",
    "# emotion lists\n",
    "emotions = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "\n",
    "# create a clahe from cv2\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "# using delib detector and predictor to get face landmarks\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "#Set the classifier as a support vector machines with polynomial kernel\n",
    "clf = SVC(kernel='linear', probability=True, tol=1e-3)\n",
    "\n",
    "data, data_labels = [], []\n",
    "\n",
    "for emotion in emotions:\n",
    "    files = glob.glob(\"dataset/%s/*\" % emotion)\n",
    "    #Append data to training and prediction list, and generate labels 0-7\n",
    "    for item in files:\n",
    "        #open image\n",
    "        image = cv2.imread(item)\n",
    "        #convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        clahe_image = clahe.apply(gray)\n",
    "        landmarks_vectorised = get_landmarks(clahe_image)\n",
    "        if landmarks_vectorised == \"error\":\n",
    "            pass\n",
    "        else:\n",
    "            #append image array to training data list\n",
    "            data.append(landmarks_vectorised)\n",
    "            data_labels.append(emotions.index(emotion))\n",
    "\n",
    "npar_train = np.array(data) #Turn the training set into a numpy array for the classifier\n",
    "npar_trainlabs = np.array(data_labels)\n",
    "clf.fit(npar_train, npar_trainlabs)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'SVC_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf2 = joblib.load('SVC_model.pkl')\n",
    "clf2.prdict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
